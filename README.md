# My-experiences-with-Hadoop-and-spark-pyspark-


To access a file from hdfs it expects the same file to be on both local system and the hdfs system.

Read a python file from spark:

PYTHONSTARTUP=tmp/input/dfa.py pyspark



>>>>>

Spark can be run independent of Hadoop, using the Mesos framework. Spark was explicitly designed to overcome the inefficiency of the MapReduce model in performing interactive and iterative computations.

Languages such as Java and C++ arenâ€™t very useful for exploratory analysis, because they lack a REPL (read-evaluate-print loop) environment for working interactively with data


>>>>>>>>
In hadoop - spark location

ls etc/spark


